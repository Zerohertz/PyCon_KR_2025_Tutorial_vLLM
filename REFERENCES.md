<h3 align="center">ğŸ“‘ Page 1. ğŸ“‘</h3>

- [1] <https://github.com/Zerohertz>

<h3 align="center">ğŸ“‘ Page 3. ğŸ“‘</h3>

- [2] <https://github.com/Zerohertz/PyCon_KR_2025_Tutorial_vLLM>

<h3 align="center">ğŸ“‘ Page 4. ğŸ“‘</h3>

- [3] <https://github.com/vllm-project/vllm/issues/1441>

<h3 align="center">ğŸ“‘ Page 7. ğŸ“‘</h3>

- [4] <https://artificialanalysis.ai/>

<h3 align="center">ğŸ“‘ Page 10. ğŸ“‘</h3>

- [5] <https://github.com/vllm-project/vllm/commit/e7d9d9c08c79b386f6d0477e87b77a572390317d>
- [6] <https://github.com/vllm-project/vllm/commit/0b98ba15c744f1dfb0ea4f2135e85ca23d572ae1>
- [7] <https://arxiv.org/abs/2309.06180>
- [8] <https://github.com/vllm-project/vllm/blob/main/LICENSE>
- [9] <https://github.com/vllm-project/vllm/issues/835>

<h3 align="center">ğŸ“‘ Page 11. ğŸ“‘</h3>

- [10] <https://www.star-history.com/>

<h3 align="center">ğŸ“‘ Page 13. ğŸ“‘</h3>

- [11] <https://github.com/vllm-project/flash-attention>
- [12] <https://github.com/flashinfer-ai/flashinfer>
- [13] <https://docs.vllm.ai/en/v0.9.2/configuration/optimization.html#chunked-prefill_1>
- [14] <https://docs.vllm.ai/en/v0.9.2/features/spec_decode.html>

<h3 align="center">ğŸ“‘ Page 15. ğŸ“‘</h3>

- [15] <https://docs.vllm.ai/en/v0.9.2/deployment/docker.html>

<h3 align="center">ğŸ“‘ Page 16. ğŸ“‘</h3>

- [16] <https://docs.vllm.ai/en/v0.9.2/serving/openai_compatible_server.html>
- [17] <https://platform.openai.com/docs/api-reference/models/list>
- [18] <https://platform.openai.com/docs/api-reference/chat/create>

<h3 align="center">ğŸ“‘ Page 17. ğŸ“‘</h3>

- [19] <https://github.com/openai/openai-python/blob/v1.97.1/src/openai/types/shared_params/function_definition.py#L13-L45>

<h3 align="center">ğŸ“‘ Page 18. ğŸ“‘</h3>

- [20] <https://docs.vllm.ai/en/v0.9.2/features/tool_calling.html>
- [21] <https://qwen.readthedocs.io/en/latest/deployment/vllm.html#parsing-tool-calls>

<h3 align="center">ğŸ“‘ Page 19. ğŸ“‘</h3>

- [22] <https://docs.vllm.ai/en/v0.9.2/features/structured_outputs.html>
- [23] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/entrypoints/openai/protocol.py#L247-L252>
- [24] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/entrypoints/openai/protocol.py#L497-L506>
- [25] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/entrypoints/openai/protocol.py#L538>
- [26] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/engine/async_llm_engine.py#L473-L484>
- [27] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/model_executor/guided_decoding/__init__.py#L100-L139>
- [28] <https://github.com/vllm-project/vllm/pull/22740>
- [29] <https://github.com/vllm-project/vllm/pull/22772>

<h3 align="center">ğŸ“‘ Page 21. ğŸ“‘</h3>

- [30] <https://docs.vllm.ai/en/v0.9.2/features/reasoning_outputs.html>
- [31] <https://qwen.readthedocs.io/en/latest/deployment/vllm.html#parsing-thinking-content>
- [32] <https://github.com/vllm-project/vllm/blob/v0.9.2/vllm/engine/arg_utils.py#L626-L634>

<h3 align="center">ğŸ“‘ Page 22. ğŸ“‘</h3>

- [33] <https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/tokenizer_config.json#L230>

<h3 align="center">ğŸ“‘ Page 23. ğŸ“‘</h3>

- [34] <https://docs.vllm.ai/en/v0.9.2/serving/openai_compatible_server.html#chat-template_1>
- [35] <https://github.com/vllm-project/vllm/tree/main/examples>

<h3 align="center">ğŸ“‘ Page 25. ğŸ“‘</h3>

- [36] <https://arxiv.org/abs/1706.03762>
- [37] <https://huggingface.co/blog/not-lain/kv-caching>
- [38] <https://docs.vllm.ai/en/v0.9.2/design/kernel/paged_attention.html>
- [39] <https://blog.vllm.ai/2023/06/20/vllm.html>
- [40] <https://github.com/HandsOnLLM/Hands-On-Large-Language-Models/blob/main/chapter03/Chapter%203%20-%20Looking%20Inside%20LLMs.ipynb>

<h3 align="center">ğŸ“‘ Page 31. ğŸ“‘</h3>

- [41] <https://www.anyscale.com/blog/continuous-batching-llm-inference>
- [42] <https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/batcher.html>

<h3 align="center">ğŸ“‘ Page 33. ğŸ“‘</h3>

- [43] <https://docs.vllm.ai/en/v0.9.2/design/arch_overview.html>
- [44] <https://docs.vllm.ai/en/v0.9.2/usage/v1_guide.html>
- [45] <https://blog.vllm.ai/2025/01/27/v1-alpha-release.html>
- [46] <https://github.com/vllm-project/vllm/issues/18571>

<h3 align="center">ğŸ“‘ Page 36. ğŸ“‘</h3>

- [47] <https://zerohertz.github.io/vllm-openai-2/#Conclusion>

<h3 align="center">ğŸ“‘ Page 38. ğŸ“‘</h3>

- [48] <https://docs.vllm.ai/en/v0.9.2/features/lora.html>
- [49] <https://arxiv.org/abs/2106.09685>

<h3 align="center">ğŸ“‘ Page 40. ğŸ“‘</h3>

- [50] <https://docs.vllm.ai/en/v0.9.2/configuration/optimization.html#parallelism-strategies>
- [51] <https://www.youtube.com/watch?v=4i76hmmnJEo>

<h3 align="center">ğŸ“‘ Page 42. ğŸ“‘</h3>

- [52] <https://docs.vllm.ai/en/v0.9.2/examples/online_serving/run_cluster.html>
- [53] <https://docs.vllm.ai/en/v0.9.2/usage/security.html>
- [54] <https://docs.pytorch.org/docs/stable/distributed.html#common-environment-variables>
- [55] <https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html#nccl-ib-disable>

<h3 align="center">ğŸ“‘ Page 43. ğŸ“‘</h3>

- [56] <https://docs.vllm.ai/en/v0.9.2/serving/distributed_serving.html>
- [57] <https://blog.vllm.ai/2025/02/17/distributed-inference.html>

<h3 align="center">ğŸ“‘ Page 44. ğŸ“‘</h3>

- [58] <https://zerohertz.github.io/distributed-computing-rdma-roce/>
- [59] <https://developer.nvidia.com/gpudirect>

<h3 align="center">ğŸ“‘ Page 45. ğŸ“‘</h3>

- [60] <https://docs.vllm.ai/en/v0.9.2/serving/distributed_serving.html#gpudirect-rdma>

<h3 align="center">ğŸ“‘ Page 46. ğŸ“‘</h3>

- [61] <https://docs.vllm.ai/en/v0.9.2/deployment/k8s.html>
- [62] <https://github.com/vllm-project/aibrix>
- [63] <https://arxiv.org/abs/2504.03648>
- [64] <https://blog.vllm.ai/2025/02/21/aibrix-release.html>
- [65] <https://aibrix.readthedocs.io/latest/>
- [66] <https://github.com/vllm-project/production-stack>
- [67] <https://docs.vllm.ai/en/v0.9.2/deployment/integrations/production-stack.html>
- [68] <https://blog.vllm.ai/2025/01/21/stack-release.html>
- [69] <https://blog.vllm.ai/production-stack/>

<h3 align="center">ğŸ“‘ Page 47. ğŸ“‘</h3>

- [70] <https://docs.vllm.ai/en/v0.9.2/deployment/frameworks/lws.html>
- [71] <https://github.com/kubernetes-sigs/lws>

<h3 align="center">ğŸ“‘ Page 48. ğŸ“‘</h3>

- [72] <https://docs.vllm.ai/en/v0.9.2/usage/metrics.html>
- [73] <https://docs.vllm.ai/en/v0.9.2/examples/online_serving/prometheus_grafana.html>

<h3 align="center">ğŸ“‘ Page 49. ğŸ“‘</h3>

- [74] <https://docs.vllm.ai/en/v0.9.2/cli/index.html#bench>
- [75] <https://docs.vllm.ai/en/v0.9.2/contributing/profiling.html>
- [76] <https://github.com/vllm-project/vllm/tree/v0.9.2/benchmarks>
- [77] <https://github.com/vllm-project/guidellm>
- [78] <https://arxiv.org/pdf/2502.06494>

<h3 align="center">ğŸ“‘ Page 50. ğŸ“‘</h3>

- [79] <https://github.com/vllm-project/vllm/discussions/11353>
- [80] <https://github.com/vllm-project/vllm/issues/17652#issuecomment-2867891239>
- [81] <https://flashinfer.ai/2024/02/02/cascade-inference.html>

<h3 align="center">ğŸ“‘ Page 52. ğŸ“‘</h3>

- [82] <https://github.com/vllm-project/vllm/issues/20336>

<h3 align="center">ğŸ“‘ Page 54. ğŸ“‘</h3>

- [83] <https://discuss.pytorch.kr/t/8-19-vllm-meetup/7401>
- [84] <https://lu.ma/cgcgprmh>
